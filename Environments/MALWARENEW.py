import numpy as np

from core import State, Reward, Environment, MeanField, Action


class Env(Environment):
    def __init__(self, is_original_dynamics: int, beta: float):
        super().__init__(is_original_dynamics, beta)
        self.name = 'malware'

        self.state_shape = 1
        self.action_shape = 1

        self.state_count = 2
        self.action_count = 2
        # hyper-parameters of the reward function
        self.k, self.l, self.q = 0.2, 0.5, 0.9

        self.state_option = [0,1]
        self.action_option = [0, 1]

        self.time_unit = 1
        self.position_unit = 1

        self.init_mf = None
        self.dim = 1


    def get_reward(self, state, action, mean_field):
        z_t = mean_field.val[1]

        reward = (-(self.k + z_t)*self.state_option[state.val[0]] -
                  self.l * self.action_option[action.val[0]])
        return Reward(reward = reward)

    def advance(self, policy, mean_field) -> MeanField:
        # init
        next_mean_field = MeanField(mean_field=None, s=self.state_count)

        # here is all the next_state
        for next_state in range(self.state_count):
            sum_next = 0
            # here is all the current state which can reach to next one
            # but for our time_unit is 5
            # not all the action that can reach this 'next_state'
            for current_state in range(self.state_count):

                sum_policy_transition = 0

                # we directly check all the valid actions
                for current_action in range(self.action_count):
                    current_state_policy = policy.val[current_state, current_action]

                    # for we have the deterministic policy
                    # our prob is 0 or 1
                    prob_transition = \
                    self.trans_prob(State(state=current_state), Action(action=current_action), mean_field)[next_state]

                    sum_policy_transition += prob_transition * current_state_policy

                # then we need to multiply it with mean field
                # print("sum_policy_transition", sum_policy_transition)
                sum_next += sum_policy_transition * mean_field.val[current_state]
                # print("sum_next", sum_next)

            next_mean_field.val[next_state] = sum_next
            # print(test_set)

        # at last, we need to normalize the output
        # Normalize the mean field values so they sum to 1
        total = np.sum(next_mean_field.val)
        if total > 0:  # Avoid division by zero
            next_mean_field.val /= total

        '''
        Now we consider a new way to update the mean_field
            1. We just focus on this place's flow. 
            2. We let last time's mean field + last place & last time's flow - current place & last time's flow
            3. In another word: last time's value + change of the flow 

        Here is the code :
            rho[i, t] = (
                rho[i][t - 1]
                + rho[i - 1, t - 1] * u[i - 1, t - 1]
                - rho[i, t - 1] * u[i, t - 1]
                )
        '''

        # So we could have
        # for next_state in range(self.state_shape):
        #     now_mean_field = mean_field[next_state]
        #     last_velocity = policy.val[]

        return next_mean_field

    def dynamics(self, state, action, mean_field):
        if int(action.val[0]) == 1:
            return State(state=0)
        else:
            n_i = 1 if np.random.rand() <= self.q else 0
            if self.is_original_dynamics == 0:

                s = int(state.val[0]) + (1-state.val[0]) * n_i
                return State(state=s)
            else:
                s = int(state.val[0]) + (1-state.val[0]) * n_i
                return State(state=s)

    def trans_prob(self, state, action, mean_field):
        if int(action.val[0]) == 1:
            return np.array([1,0])
        else:
            prob = np.zeros(self.state_count)
            n_i = 1 if np.random.rand() <= self.q else 0
            if self.is_original_dynamics == 0:

                s = int(state.val[0]) + (1 - state.val[0]) * n_i
                prob[s] = 1
            else:
                s = int(state.val[0]) + (1 - state.val[0]) * n_i
                prob[s] = 1
            return prob
