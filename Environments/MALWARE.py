import numpy as np

from core import State, Reward, Environment, MeanField


class Env(Environment):
    def __init__(self, is_original_dynamics: int, beta: float):
        super().__init__(is_original_dynamics, beta)
        self.name = 'malware'

        self.state_shape = 1
        self.action_shape = 1

        self.state_count = 10
        self.action_count = 2
        # hyper-parameters of the reward function
        self.k, self.l = 0.2, 0.5

        self.state_option = [i for i in range(self.state_count)]
        self.action_option = [0, 1]

        self.time_unit = 1
        self.position_unit = 1

        self.init_mf = None
        self.dim = 1


    def get_reward(self, state, action, mean_field):
        mean = self.mean_level(mean_field)
        reward = -(self.k + mean / 10) * state.val / 10 - self.l * action.val
        return Reward(reward = reward)

    def mean_level(self, mean_field):
        mean = 0.0
        for quality_level in range(self.state_count ):
            mean += quality_level * mean_field.val[quality_level]
        return mean

    def advance(self, policy, mean_field):
        next_mean_field = MeanField(mean_field=None, s=self.state_count )
        if self.is_original_dynamics == 0:
            for level in range(self.state_count ):
                none_prob = policy.val[level, 0] / (self.state_count - level)
                repair_prob = policy.val[level, 1]
                next_mean_field.val[0] += mean_field.val[level] * repair_prob
                for s in range(level, self.state_count ):
                    next_mean_field.val[s] += mean_field.val[level] * none_prob
        else:
            for level in range(self.state_count ):
                repair_prob = policy.val[level, 1]
                next_mean_field.val[0] += mean_field.val[level] * repair_prob
                none_prob = policy.val[level, 0] / (self.state_count - level)
                if level == self.state_count - 1:
                    next_mean_field.val[level] += mean_field.val[level] * none_prob

                counter = self.state_count - level
                for s in range(level, self.state_count - 1):
                    if counter == 0:
                        break
                    if counter == 1:
                        next_mean_field.val[s] += mean_field.val[level] * none_prob * 1
                        counter -= 1
                    else:
                        next_mean_field.val[s] += mean_field.val[level] * none_prob * 2
                        counter -= 2
        # update world mean field
        return next_mean_field

    def dynamics(self, state, action, mean_field):
        if int(action.val[0]) == 1:
            return State(state=0)
        else:
            if self.is_original_dynamics == 0:
                s = int(state.val[0]) + int( np.random.choice(np.array(range(0, self.state_count - state.val[0] ))) )
                return State(state=s)
            else:
                x = np.random.uniform(0.5, 1.0 + 1e6)
                s = int(state.val[0]) + int(x * (self.state_count - state.val[0]))
                return State(state=s)

    def trans_prob(self, state, action, mean_field):
        if int(action.val[0]) == 1:
            return np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
        else:
            prob = np.zeros(10)
            if self.is_original_dynamics == 0:
                for i in range(int(state.val[0]), self.state_count ):
                    prob[i] = 1.0 / (self.state_count - int(state.val[0]))
            else:
                num = (self.state_count - int(state.val[0])) / 2.0
                for i in range(int(state.val[0]), int(state.val[0]) + int(num)):
                    prob[i] = 2.0 / (self.state_count - int(state.val[0]))
                if num > int(num):
                    prob[int(state.val[0]) + int(num)] = 1.0 / (self.state_count - int(state.val[0]))
            return prob
